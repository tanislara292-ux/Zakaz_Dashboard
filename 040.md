Задача для дев-сервера: Полный пересброс и проверка QTickets‑стека

Цель: восстановить “чистое” состояние инфраструктуры и повторно построить весь контур (ClickHouse + образ qtickets_api) по свежему коду из main, убедившись, что dry-run и ручной запуск loader’а отрабатывают без ошибок.

Исходные условия: работать в редакторе облачной dev‑среды (SSH не требуется); предполагается, что git-репозиторий Zakaz_Dashboard уже размонтирован (как минимум read-write) и доступна локальная Docker‑установка.

1. Обновить исходники

Перейти в корень репо Zakaz_Dashboard.
Выполнить:
git fetch --all
git checkout main
git pull --ff-only origin main
Убедиться, что рабочее дерево чистое: git status → никаких локальных изменений, иначе остановиться и запросить уточнения.
2. Полностью очистить стек ClickHouse / QTickets

Остановить любые контейнеры проекта:
docker ps --all --format '{{.Names}}' | grep -E 'qtickets|ch-zakaz' || true
docker stop ch-zakaz || true
docker rm ch-zakaz || true
docker ps --all --format '{{.Names}}' | grep qtickets && docker rm $(docker ps -aq --filter name=qtickets) || true
Снести старые docker-compose ресурсы:
cd dashboard-mvp/infra/clickhouse
docker compose down --volumes --remove-orphans || true
docker network prune --force
Удалить старые имиджи интеграции и кэш:
docker images --format '{{.Repository}}:{{.Tag}} {{.ID}}' | grep qtickets_api || true
docker rmi qtickets_api:latest || true
docker builder prune --force
Очистить каталоги данных ClickHouse:
rm -rf data logs caddy_data || true
mkdir -p data logs
chown -R 101:101 data logs
Очистить env-файлы:
Убедиться, что infra/clickhouse/.env и configs/.env.qtickets_api.sample существуют.
Если .env отсутствует, скопировать из .env.example.
Env‑файл для интеграции подготовим позже.
3. Поднять ClickHouse с нуля

Запустить скрипт bootstrap:
cd dashboard-mvp/infra/clickhouse
../../scripts/bootstrap_clickhouse.sh
Скрипт обязан: создать каталоги, поднять ch-zakaz, дождаться healthcheck, применить bootstrap_schema.sql, вывести список таблиц. Проверить в выводе, что присутствуют:
stg_qtickets_api_orders_raw
stg_qtickets_api_inventory_raw
fact_qtickets_sales_daily
fact_qtickets_inventory_latest
mv_qtickets_sales_latest
meta_job_runs
v_qtickets_sales_dashboard
Дополнительно вручную проверить:
docker exec ch-zakaz clickhouse-client \
  --user=admin --password=admin_pass \
  -q "SELECT count() FROM zakaz.meta_job_runs;"
Ожидаем 0 (пустая база).
4. Пересобрать образ интеграции

Подготовить env-файл:

cd /opt/zakaz_dashboard/Zakaz_Dashboard/dashboard-mvp
cp configs/.env.qtickets_api.sample /tmp/.env.qtickets_api.dev
Внутри файла оставить DRY_RUN=true, QTICKETS_TOKEN/ORG_NAME — тестовые заглушки (не пустые строки).
Проверить переменные: CLICKHOUSE_HOST=ch-zakaz, CLICKHOUSE_PORT=8123, CLICKHOUSE_DB=zakaz, CLICKHOUSE_USER=admin, CLICKHOUSE_PASSWORD=admin_pass.
Собрать образ:

docker build \
  -f integrations/qtickets_api/Dockerfile \
  -t qtickets_api:latest \
  .
5. Прогнать smoke-скрипт

Выполнить:
./scripts/smoke_qtickets_dryrun.sh /tmp/.env.qtickets_api.dev
Скрипт должен:
Пересобрать образ (если не сделал ранее),
Запустить контейнер,
Проверить exit code (ожидаем 0),
Вывести последние строки логов,
Запросить таблицу zakaz.meta_job_runs и подтвердить, что записей нет (Dry Run).
После скрипта вручную двойная проверка:
docker exec ch-zakaz clickhouse-client \
  --user=admin --password=admin_pass \
  -q "SELECT count() FROM zakaz.meta_job_runs;"
Должно остаться 0.
6. Ручной запуск loader’а (дополнительная проверка)

Запустить контейнер вручную:
docker run --rm \
  --network clickhouse_default \
  --env-file /tmp/.env.qtickets_api.dev \
  --name qtickets_api_dryrun \
  qtickets_api:latest
Ожидаемые логи:
“stub mode” или “Fetching orders via GET…” → при DRY_RUN и заглушке API должны быть заглушки.
Ни одного Traceback.
Лог “Dry-run: no ClickHouse writes…” (если предусмотрено).
После завершения контейнера снова проверить meta_job_runs и staging:
docker exec ch-zakaz clickhouse-client \
  --user=admin --password=admin_pass \
  -q "SELECT count() FROM zakaz.stg_qtickets_api_orders_raw;"
docker exec ch-zakaz clickhouse-client \
  --user=admin --password=admin_pass \
  -q "SELECT count() FROM zakaz.meta_job_runs;"
Оба значения 0.
7. Протестировать штатный GET‑путь и fallback (моками)

Внутри репо выполнить pytest юнит‑набор для клиента:
poetry run pytest integrations/qtickets_api/tests/test_client.py
Если poetry не используется на дев-сервере, запустить через python -m pytest ... с установленными зависимостями.
Убеждаемся, что 4/4 теста проходят; если модульная среда отсутствует, зафиксировать факт и запросить инструкции.
8. Боевой пробный запуск (опционально)

Если предоставлен боевой токен и DRY_RUN=false, повторить шаг 6 с обновлённым env: проверить, что meta_job_runs пополнилась записью status=ok и количество строк fact_qtickets_sales_daily > 0.
В любом случае не менять prod‑переменные без явного разрешения; если токена нет, описать, что боевой тест не проводился.
9. Очистка временных файлов

Удалить /tmp/.env.qtickets_api.dev (если не требуется), чтобы не оставлять секреты.
Проверить, что рабочая директория репо чистая (git status).
10. Что предоставить по результату

Краткий отчёт (Markdown) в logs/ или docs/ (по принятому шаблону), который содержит:
выполненные шаги,
версии образов (docker images qtickets_api:latest → ID + дата),
выводы smoke-скрипта (exit code, фрагменты логов, SHOW TABLES),
результаты SQL-проверок (count() и meta_job_runs).
Если что-то не выполнилось → приложить подробный лог и стопнуть план.
Definition of Done

bootstrap_clickhouse.sh завершился успешно; таблицы перечисленные в шаге 3 присутствуют.
Образ qtickets_api:latest пересобран, smoke‑скрипт прошёл (exit code 0).
Dry-run не оставляет записей в ClickHouse.
Ручной запуск контейнера тоже завершается без исключений.
Юнит-тесты клиента проходят.
Отчёт с логами и SQL-просмотром передан.
Следовать строго; при первой ошибке фиксировать вывод и заканчивать выполнение, чтобы привлечь внимание.